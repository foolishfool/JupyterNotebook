{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1a67a0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import random\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score, f1_score, roc_auc_score, recall_score, precision_score\n",
    "import cv2\n",
    "from sklearn import metrics\n",
    "import newSom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "170bfa3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stringToiInt(X,name):\n",
    "    X[name] = X[name].astype(str).str.strip()\n",
    "    #print(X[name])\n",
    "    for i in range(0, X.shape[0]):\n",
    "        if  X.at[i,name]== 'Jan':\n",
    "            X.at[i,name] =1            \n",
    "        elif  X.at[i,name]== 'Feb':\n",
    "                X.at[i,name]=2 \n",
    "        elif  X.at[i,name]== 'Mar': \n",
    "            X.at[i,name]=3 \n",
    "        elif  X.at[i,name]== 'Apr': \n",
    "            X.at[i,name]=4 \n",
    "        elif  X.at[i,name]== 'May': \n",
    "            X.at[i,name]=5 \n",
    "        elif  X.at[i,name]== 'June': \n",
    "            X.at[i,name]=6  \n",
    "        elif  X.at[i,name]== 'Jul': \n",
    "            X.at[i,name]=7 \n",
    "        elif  X.at[i,name]== 'Aug':\n",
    "            X.at[i,name]=8 \n",
    "        elif  X.at[i,name]== 'Sep':\n",
    "            X.at[i,name]=9 \n",
    "        elif  X.at[i,name]== 'Oct':\n",
    "            X.at[i,name]=10\n",
    "        elif  X.at[i,name]== 'Nov':\n",
    "            X.at[i,name]=11 \n",
    "        elif  X.at[i,name]== 'Dec': \n",
    "            X.at[i,name]=12\n",
    "        elif  X.at[i,name]== 'Returning_Visitor':  \n",
    "            X.at[i,name]=0\n",
    "        elif  X.at[i,name]== 'New_Visitor': \n",
    "            X.at[i,name]=1\n",
    "        elif  X.at[i,name]== 'Other': \n",
    "            X.at[i,name]=3\n",
    "        elif  X.at[i,name]== 'False': \n",
    "            X.at[i,name]=0        \n",
    "        elif  X.at[i,name]== 'True': \n",
    "            #print(X.at[i,name])\n",
    "            X.at[i,name]=1\n",
    "            #print(X.at[i,name])\n",
    "        else: \n",
    "            print(\"Unkonwn value {}\".format(X.at[i,name]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "1bea7976",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(5000, 16)\n",
      "(5000, 15)\n"
     ]
    }
   ],
   "source": [
    "csv = pd.read_csv(\"online_shoppers_intention.csv\")\n",
    "del csv[\"Month\"]\n",
    "del csv[\"Weekend\"]\n",
    "stringToiInt(csv,\"VisitorType\")\n",
    "stringToiInt(csv,\"Revenue\")\n",
    "#sample number of train and test dataset\n",
    "train_num = 5000\n",
    "test_num = 5000\n",
    "#get samples\n",
    "data_train = csv.sample(train_num)\n",
    "data_test = csv.sample(test_num)\n",
    "data_train = data_train.to_numpy()\n",
    "data_test = data_test.to_numpy()\n",
    "\n",
    "label_train = data_train[:,data_train.shape[1]-1]\n",
    "label_test = data_test[:,data_test.shape[1]-1]\n",
    "# delete the last column\n",
    "data_train= data_train[:,:-1]\n",
    "data_test= data_test[:,:-1]\n",
    "#print(label_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "3e691f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormalizeLables(X,Y,category = 0):\n",
    "    \"\"\"\n",
    "    transfrer predcited label to match the range of true label\n",
    "    a = np.amax(X)+1,b = np.amax(Y)+1 the number of classes in X and Y\n",
    "    b= m*n,  to make it easier , to make b can be divided by a\n",
    "    category = 0 noramlize label\n",
    "    category = 1 normalize sub label\n",
    "    \"\"\"\n",
    "    div = int((np.amax(Y)+1)/(np.amax(X)+1))\n",
    "    #print(\"div : {}\".format(div) )\n",
    "    if(category == 0):\n",
    "        global nLabel \n",
    "        nLabel = np.arange(Y.size)\n",
    "    if(category == 1):\n",
    "        global nsubLabel \n",
    "        nsubLabel = np.arange(Y.size)\n",
    "    index = 0;\n",
    "    for idx, y in enumerate(Y): \n",
    "       # print(\"idx {}\".format(idx))\n",
    "        for i in range(1,div+1):\n",
    "            if(y < i*div):\n",
    "                index+=1\n",
    "                #print(i-1)\n",
    "                if(category == 0):\n",
    "                    nLabel[idx] = i-1\n",
    "                if(category == 1):\n",
    "                    nsubLabel[idx] = i-1\n",
    "                break               \n",
    "    if(category == 0):\n",
    "        print(\"normalized predicted nLabel:\\n {} \".format(nLabel))\n",
    "    if(category == 1):\n",
    "        print(\"normalized predicted nsubLabel: \\n{} \".format(nsubLabel))\n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "5076323a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def purity_score(y_true, y_pred):\n",
    "    # compute contingency matrix (also called confusion matrix)\n",
    "    #print(y_true.shape)\n",
    "    #print(y_pred.shape)\n",
    "    contingency_matrix = metrics.cluster.contingency_matrix(y_true, y_pred)\n",
    "    # return purity\n",
    "   # print (np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix))\n",
    "    return np.sum(np.amax(contingency_matrix, axis=0)) / np.sum(contingency_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9993a065",
   "metadata": {},
   "outputs": [],
   "source": [
    "def groupClusterList(m,Y):\n",
    "    \"\"\"\n",
    "    transfer all label in a list\n",
    "    m: cluster Number\n",
    "    \"\"\"\n",
    "    clusters = []\n",
    "    for i in range(0,m):\n",
    "        newlist = []\n",
    "        for idx, y in enumerate(Y): \n",
    "            if(y == i):\n",
    "                newlist.append(idx)\n",
    "        clusters.append(newlist) \n",
    "   # print(clusters)\n",
    "    return clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7cc2ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getNoisyClusters(list_true,list_pred):\n",
    "    \"\"\"\n",
    "    get the wrong data indices in the training data when predicted with weight1\n",
    "    \"\"\"\n",
    "    noisylist = []\n",
    "    for i in range(0,len(list_true)):\n",
    "        newlist = [item for item in list_pred[i] if item not in list_true[i]]\n",
    "        noisylist.append(newlist)\n",
    "    print(\"noisy data:\")\n",
    "    return noisylist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "6a7b3461",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_noisy_data(noisy_list, percent):\n",
    "    newlist = []\n",
    "    for x in noisy_list:\n",
    "        for e in x:\n",
    "            newlist.append(e)\n",
    "   # print(newlist)\n",
    "   # print(len(newlist))\n",
    "   # print(int(percent * len(newlist)))\n",
    "    newlist = random.sample(newlist, int(percent * len(newlist)))\n",
    "   # print(newlist)\n",
    "    return newlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "97037b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce_data_byindice(reduced_indices,X,category = 0):\n",
    "   \n",
    "    if(category == 0): \n",
    "        global df_subset\n",
    "        df_subset=X.drop(reduced_indices)\n",
    "        print(\"reduced_indices :{}\".format(reduced_indices))\n",
    "    if(category == 1): \n",
    "        global df_sublabel\n",
    "        df_sublabel=X.drop(reduced_indices)\n",
    "    #print(df_subset.to_numpy())   \n",
    "    return "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "f6114245",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "som1 = newSom.SOM(m=4, n=4, dim=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "373f3384",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (16,16) (16,15) ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-48-9d6a82af176c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msom1\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdata_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32mD:\\cygwin64\\home\\fooli\\SOM\\newSom.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, weightIndex, epochs, shuffle)\u001b[0m\n\u001b[0;32m    199\u001b[0m                 \u001b[0minput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0midx\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    200\u001b[0m                 \u001b[1;31m# Do one step of training\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 201\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minput\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    202\u001b[0m                 \u001b[1;31m# Update learning rate\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    203\u001b[0m                 \u001b[0mglobal_iter_counter\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\cygwin64\\home\\fooli\\SOM\\newSom.py\u001b[0m in \u001b[0;36mstep\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    107\u001b[0m         \u001b[1;31m# x_stack , with mxn row , each row has the same array: x\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    108\u001b[0m         \u001b[1;31m# Get index of best matching unit\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 109\u001b[1;33m         \u001b[0mbmu_index\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_find_bmu\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    110\u001b[0m         \u001b[1;31m#print(\"bmu_index{}\".format(bmu_index));\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    111\u001b[0m         \u001b[1;31m# Find location of best matching unit, _locations is all the indices for a given matrix for array\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\cygwin64\\home\\fooli\\SOM\\newSom.py\u001b[0m in \u001b[0;36m_find_bmu\u001b[1;34m(self, x, newWeights)\u001b[0m\n\u001b[0;32m     90\u001b[0m         \u001b[1;31m#print(weights.shape)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m         \u001b[1;31m#print(\"x_stack - newWeights:{}\".format(x_stack - newWeights ))   #, axis =1  process by row\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 92\u001b[1;33m         \u001b[0mdistance\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlinalg\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnorm\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx_stack\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mnewWeights\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     93\u001b[0m         \u001b[1;31m#print(\"distance:{}\".format(distance ))\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     94\u001b[0m        \u001b[1;31m# print(\"min distance:\")\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: operands could not be broadcast together with shapes (16,16) (16,15) "
     ]
    }
   ],
   "source": [
    "som1.fit(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c16ff160",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights used:\n",
      "\n",
      "[[5.         3.5        1.6        0.6       ]\n",
      " [5.22428465 3.30754573 2.33855975 0.72960205]\n",
      " [5.67514031 4.12089664 1.92528117 0.56190278]\n",
      " [5.3367624  3.12385155 2.84976911 0.92516284]\n",
      " [5.6052653  2.8947347  3.76166179 1.2052653 ]\n",
      " [5.8712891  3.07491509 4.23395593 1.5721077 ]\n",
      " [6.16725902 3.03078829 4.87739809 1.72518126]\n",
      " [6.07200026 3.18143338 5.02494718 1.98593094]\n",
      " [5.94497552 2.99108121 5.12186897 2.25017405]]\n",
      "predicted label:\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 4 6 5 6 3 6 4 4 5 4 6 4 6 5 4 6 4 6 5 6 6\n",
      " 5 6 6 6 5 4 4 4 4 6 5 5 6 5 4 4 4 6 4 4 4 5 5 5 3 4 8 8 7 7 8 7 5 7 6 7 7\n",
      " 6 7 8 8 7 7 7 8 6 7 8 7 6 7 7 6 6 8 6 7 7 8 6 6 7 8 7 6 7 8 7 8 8 8 7 6 7\n",
      " 7 7]\n"
     ]
    }
   ],
   "source": [
    "predictions1_w1 = som1.predict(iris_data,som1.weights1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3fe08705",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "normalized predicted nLabel:\n",
      " [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 2 1 2 1 1 2 1 2 1 2 2\n",
      " 1 2 2 2 1 1 1 1 1 2 1 1 2 1 1 1 1 2 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2] \n"
     ]
    }
   ],
   "source": [
    "NormalizeLables(iris_label,predictions1_w1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "fa3c796e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8733333333333333"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "purity_score(iris_label,nLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d63addf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy data:\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[[],\n",
       " [106],\n",
       " [50, 51, 52, 54, 56, 58, 63, 65, 68, 70, 72, 73, 75, 76, 77, 83, 86, 91]]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "getNoisyClusters(groupClusterList(3,iris_label),groupClusterList(3,nLabel))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "225aff9a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "noisy data:\n",
      "reduced_indices :[91]\n"
     ]
    }
   ],
   "source": [
    "global noisy_list\n",
    "noisy_list = getNoisyClusters(groupClusterList(3,iris_label),groupClusterList(3,nLabel))\n",
    "reduced_indices = reduce_noisy_data(noisy_list,0.1)\n",
    "reduce_data_byindice(reduced_indices,df)\n",
    "reduce_data_byindice(reduced_indices,df_label,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "04dad815",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generate weight2:\n",
      " [[4.8        3.1        1.6        0.2       ]\n",
      " [5.19128492 3.29640995 2.39526155 0.54959758]\n",
      " [6.38579405 3.3846698  5.05850548 1.75947421]\n",
      " [5.13644368 3.20763213 2.35467289 0.53367227]\n",
      " [5.66495852 2.97297027 3.55967955 1.01070762]\n",
      " [5.99315771 3.1079102  4.7315512  1.75715626]\n",
      " [5.63979241 2.70605695 3.83804691 1.19183923]\n",
      " [6.38118823 2.90937865 4.26306841 1.2845572 ]\n",
      " [6.15326568 2.77746452 4.73086032 1.56214044]]\n"
     ]
    }
   ],
   "source": [
    "som1.fit(df_subset.to_numpy(),weightIndex =1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "5fede6ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights used:\n",
      "\n",
      "[[4.8        3.1        1.6        0.2       ]\n",
      " [5.19128492 3.29640995 2.39526155 0.54959758]\n",
      " [6.38579405 3.3846698  5.05850548 1.75947421]\n",
      " [5.13644368 3.20763213 2.35467289 0.53367227]\n",
      " [5.66495852 2.97297027 3.55967955 1.01070762]\n",
      " [5.99315771 3.1079102  4.7315512  1.75715626]\n",
      " [5.63979241 2.70605695 3.83804691 1.19183923]\n",
      " [6.38118823 2.90937865 4.26306841 1.2845572 ]\n",
      " [6.15326568 2.77746452 4.73086032 1.56214044]]\n",
      "predicted label:\n",
      " [0 0 0 0 0 1 0 0 0 0 0 0 0 0 0 1 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 1 0 0 0 0 0 2 7 2 6 8 8 5 6 7 6 6 7 6 8 4 7 5 6 8 6 5 7 8 8\n",
      " 7 7 8 2 8 4 6 6 6 8 5 5 7 7 6 6 6 6 6 6 6 6 7 4 6 2 5 2 2 2 2 6 2 2 2 2 8\n",
      " 2 8 5 2 2 2 2 8 2 5 2 8 2 2 8 5 2 2 2 2 2 8 8 2 2 2 5 2 2 2 5 2 2 2 8 2 2\n",
      " 5]\n",
      "normalized predicted nsubLabel: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 0 2 0 2 2 2 1 2 2 2 2 2 2 2 1 2 1 2 2 2 1 2 2 2\n",
      " 2 2 2 0 2 1 2 2 2 2 1 1 2 2 2 2 2 2 2 2 2 2 2 1 2 0 1 0 0 0 0 2 0 0 0 0 2\n",
      " 0 2 1 0 0 0 0 2 0 1 0 2 0 0 2 1 0 0 0 0 0 2 2 0 0 0 1 0 0 0 1 0 0 0 2 0 0\n",
      " 1] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.6442953020134228"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2_W2 = som1.predict(df_subset.to_numpy(),som1.weights2)\n",
    "NormalizeLables(df_sublabel,predictions2_W2,1)\n",
    "purity_score(df_sublabel,nsubLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a02751e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "weights used:\n",
      "\n",
      "[[5.         3.5        1.6        0.6       ]\n",
      " [5.22428465 3.30754573 2.33855975 0.72960205]\n",
      " [5.67514031 4.12089664 1.92528117 0.56190278]\n",
      " [5.3367624  3.12385155 2.84976911 0.92516284]\n",
      " [5.6052653  2.8947347  3.76166179 1.2052653 ]\n",
      " [5.8712891  3.07491509 4.23395593 1.5721077 ]\n",
      " [6.16725902 3.03078829 4.87739809 1.72518126]\n",
      " [6.07200026 3.18143338 5.02494718 1.98593094]\n",
      " [5.94497552 2.99108121 5.12186897 2.25017405]]\n",
      "predicted label:\n",
      " [0 0 0 0 0 2 0 0 0 0 0 0 0 0 2 2 0 0 2 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 6 6 6 4 6 5 6 3 6 4 4 5 4 6 4 6 5 4 6 4 6 5 6 6\n",
      " 5 6 6 6 5 4 4 4 4 6 5 5 6 5 4 4 4 4 4 4 5 5 5 3 4 8 8 7 7 8 7 5 7 6 7 7 6\n",
      " 7 8 8 7 7 7 8 6 7 8 7 6 7 7 6 6 8 6 7 7 8 6 6 7 8 7 6 7 8 7 8 8 8 7 6 7 7\n",
      " 7]\n",
      "normalized predicted nsubLabel: \n",
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 2 2 2 1 2 1 2 1 2 1 1 1 1 2 1 2 1 1 2 1 2 1 2 2\n",
      " 1 2 2 2 1 1 1 1 1 2 1 1 2 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 1 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2] \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8791946308724832"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions2_W1 = som1.predict(df_subset.to_numpy(),som1.weights1)\n",
    "NormalizeLables(df_sublabel,predictions2_W1,1)\n",
    "purity_score(df_sublabel,nsubLabel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84b2c895",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
