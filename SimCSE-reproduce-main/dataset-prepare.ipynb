{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ba9ce04e",
   "metadata": {},
   "source": [
    "因为map dataset需要时间比较久（~15min），因此单独进行，然后保存下来，训练的时候只需要直接load 已经map好的dataset就可以了。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "found-upset",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from datasets import Dataset,load_dataset\n",
    "from transformers import BertTokenizer, BertModel, AdamW, get_linear_schedule_with_warmup\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "looking-buyer",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713f06ac",
   "metadata": {},
   "source": [
    "这里和原代码稍有不同，因为直接使用load_dataset方法load txt文件在我的环境里会有问题，不报错也不运行。因此改成了通过pandas中转一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04c8616",
   "metadata": {},
   "source": [
    "根据原代码里给出的wiki句子数据集的下载路径下好数据集（ https://huggingface.co/datasets/princeton-nlp/datasets-for-simcse/resolve/main/wiki1m_for_simcse.txt ），然后 wiki_text_file = 你的数据集路径。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "looking-tradition",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                        0\n",
      "0                                 YMCA in South Australia\n",
      "1       South Australia (SA)  has a unique position in...\n",
      "2       The compound of philosophical radicalism, evan...\n",
      "3       It was into this social setting that in Februa...\n",
      "4       for apprentices and others, after their day's ...\n",
      "...                                                   ...\n",
      "995442                                  Rubaschow: Roman.\n",
      "995443                 Typoskript, März 1940, 326 pages.\"\n",
      "995444  He deemed the discovery important because \"\"Da...\n",
      "995445  In 2018, he reported that Elsinor Verlag (publ...\n",
      "995446  He also reported a new English translation to ...\n",
      "\n",
      "[995447 rows x 1 columns]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['text'],\n",
       "    num_rows: 995447\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# use pandas to read simCSE-wiki.txt\n",
    "wiki_text_file = 'wiki1m_for_simcse.txt'\n",
    "wiki = pd.read_csv(wiki_text_file,sep = '\\t',header = None)\n",
    "print(wiki)\n",
    "wiki.columns = ['text']\n",
    "# use Dataset.from_pandas to convert panda dataframe to hugging face dataset\n",
    "wiki_dataset = Dataset.from_pandas(wiki,split= \"train\")\n",
    "wiki_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "threatened-strand",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_features(examples):\n",
    "    \n",
    "    total = len(examples['text'])\n",
    "    # total = batch_size\n",
    "    \n",
    "    # Avoid \"None\" fields \n",
    "    for idx in range(total):\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "        if examples['text'][idx] is None:\n",
    "            examples['text'][idx] = \" \"\n",
    "\n",
    "    sentences = examples['text'] + examples['text']\n",
    "\n",
    "    # set max_length here:\n",
    "    sent_features = tokenizer(sentences, max_length=32, truncation=True, padding=\"max_length\")\n",
    "\n",
    "    features = {}\n",
    "    for key in sent_features:\n",
    "        features[key] = [[sent_features[key][i], sent_features[key][i+total]] for i in range(total)]\n",
    "    \n",
    "    return features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "internal-capability",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb245f195ead4ec9861f3ccd4f646658",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/995447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_dataset = wiki_dataset.map(prepare_features,batched=True, remove_columns=['text'], batch_size=4000) \n",
    "#apply the prepare_features function to the entire dataset:\n",
    "#take about 15 min"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f1fe35bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.set_format(type='torch', columns=['input_ids', 'token_type_ids', 'attention_mask'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "hispanic-jersey",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask'],\n",
       "    num_rows: 995447\n",
       "})"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "rotary-lincoln",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4ab39794837f4c7a9b61ab79d97e508a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Saving the dataset (0/1 shards):   0%|          | 0/995447 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# save to disk for reuse\n",
    "train_dataset.save_to_disk(\"wiki_for_sts_32\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "driven-choir",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
