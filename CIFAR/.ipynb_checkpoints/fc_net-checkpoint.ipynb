{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f1c7ad9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da0bac43",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TwoLayerNet(object):\n",
    "    \n",
    "    def __init__(self, input_dim = 3*32*32, hidden_dim = 100, num_classes = 10, weight_scale = 1e-3, reg = 0.0):\n",
    "        \"\"\"\n",
    "        - dropout : scalar between 0 and 1 giving dropout strength.\n",
    "        - weight_scale : Scalar giving the standard deviation for random initialization of the weights, normally it should not be too big, so 1e-3 = 1*10^-3\n",
    "        - reg: Scalar giving L2 regularization strength lamda \n",
    "        \"\"\"\n",
    "        \n",
    "        self.params = {}\n",
    "        self.reg = reg\n",
    "        self.params['W1'] = weight_scale * np.random.randn(input_dim,hidden_dim) \n",
    "        # 2D array with input_dim rows and hidden_dim columns, the values applies is randomized with standrad deviation distribution\n",
    "        self.params['b1'] = np.zeros(1, hidden_dim)\n",
    "        # 2D array with 1 row, hidden_dim columns\n",
    "        self.params['W2'] = weight_scale * np.random.randn(hidden_dim, num_classes)\n",
    "        self.params['b2'] = np.zeros(1, num_classses)\n",
    "        \n",
    "    def loss(self, X, y = None)\n",
    "        \"\"\"\n",
    "        -X: array of input data of shape (N, d_1,..., d_k)\n",
    "        -y: array of lables, of shape(N,) y[i]gives the label for X[i]\n",
    "        If y is none, then run a test-time forward pass of the model and return\n",
    "        - scores: array of shape(N,c) giveing classfication scores, scores[i, c] is the classfication score for X[i] and class c\n",
    "        If y is not none, then run a training-time forward pass of the model and return a tuple of \n",
    "        -loss: Scalar value giving the loss\n",
    "        - grads: Dictionary with the same keys as self.params, mapping parameter names to gradients of the loss with repsect to those parameters\n",
    "        \"\"\"\n",
    "        \n",
    "        scores = None\n",
    "        N = X.shape[0]\n",
    "        W1,b1 = self.params['W1'], self.params['b1']\n",
    "        W2,b2 = self.params['W2'], self.params['b2']\n",
    "        out, cache = affine_relu_forward(X,W1,b1)\n",
    "        scores = out\n",
    "        \n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
