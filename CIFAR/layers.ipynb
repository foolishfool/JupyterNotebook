{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2f68da5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd1a2a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_forward(x,w,b):\n",
    "    \"\"\"\n",
    "        Computes the foward pass for an affine (fully_connected) layer\n",
    "        -x A numpy array containing input data of shape(N, d_1,...,d_k), N is the minibatch， D is the column number\n",
    "        -w a numpy array of weights, of shape(D, M)\n",
    "        -b a numpy array of bias, of shape(M,) one dimension array, the length is M, could be both row vector or column vector\n",
    "        return a tuple of :\n",
    "        -out : output of shape(N,M)\n",
    "         np.dot(x，w) -> (N,M) \n",
    "         np.dot(x，w) + b ?  -> #(N,M)\n",
    "        -cache: (x,w,b)\n",
    "        \"\"\"\n",
    "    out = None\n",
    "    # reshape x into raws\n",
    "    N = x.shape[0]\n",
    "    x_row = x.reshape(N,-1)  #(N,D)  -1 means the colomns number is auto adjusted, w is D so it is D\n",
    "    out = np.dot(x_row,w) + b #(N,M) based on equation, b is a column vector (1,M)\n",
    "    cache = (x,w,b)\n",
    "    \n",
    "    return out, cache\n",
    "\n",
    "def affine_backforward(dout, cache)\n",
    "    \"\"\"\n",
    "        computes the backward pass for an affine layer\n",
    "        -dout upstream derivate , of shape (N,M)\n",
    "        -cache ：tuple of \n",
    "        -x input data, of shape (N, d_1,...d_k)\n",
    "        - w: weights of shape(D,M)\n",
    "        Return a tuple of\n",
    "        -dx: Gradient with respect to x, of shape(N, d1, ..., d_k)\n",
    "        -dw: Gradient with respect to w, of shape (D,M)\n",
    "        -db: Gradient with respect to b, of shape (M,)\n",
    "        \"\"\"\n",
    "        x, w, b= cache\n",
    "        dx,dw, db = None, None, None\n",
    "        dx = np.dot(dout, w.T)\n",
    "        dx = np.reshape(dx, x.shape)\n",
    "        x_row= np.dot(x.shape[0],-1)\n",
    "        dw = np.dot(x_row.T, dout)\n",
    "        db = np.sum(dout, axis=0, keepdims = True)   #(1,M)\n",
    "        \n",
    "        return dx, dw, db\n",
    "    \n",
    "def softmax_loss (x,y):\n",
    "    \"\"\"\n",
    "    computes the loss and gradient for softmax classification\n",
    "    Inputs:\n",
    "    -x input data, of shape(N,C), where x[i,j] is the score for the jth class for  the ith input\n",
    "    - y: vector of labels, of shape(N,) where y[i] is the label for x[i] and 0<= y[i] <C\n",
    "    return a tuple of :\n",
    "    -loss, Scalar giving the loss\n",
    "    -dx gradient of the loass with respect to x\n",
    "    \"\"\"\n",
    "    probs = np.exp(x- np.max(x, axis =1, keepdims = True))  #axxis =1 means get max value aong row direction\n",
    "    probs /= np.sum(probs, axis=1, keepdims = True)\n",
    "    N = x.shape[0]\n",
    "    loss = -np.sum(np.log(probs[np.arrange(N),y]))/N\n",
    "    dx = probs.copy()\n",
    "    dx[np.arrange(N),y] == 1\n",
    "    dx/= N\n",
    "    \n",
    "    return loss, dx\n",
    "\n",
    "def ReLU(x):\n",
    "    return np.maximum(0,x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
